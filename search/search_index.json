{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome # Wat iz dis? # Well this is just a little site I wrote in my spare time to hopefully help other people with various things. I'm not a professional programmer or anything, I just like to tinker with things and learn new things. I'm not a professional writer either, so I apologize for any spelling or grammar mistakes. If you find any mistakes/improvements regarding text and ofc the content, create a git issue here -> click me What can I find here? # All sorts of tips little tools and howto's for technical stuff. Just look around in the tabs above and on the left.","title":"Welcome"},{"location":"#welcome","text":"","title":"Welcome"},{"location":"#wat-iz-dis","text":"Well this is just a little site I wrote in my spare time to hopefully help other people with various things. I'm not a professional programmer or anything, I just like to tinker with things and learn new things. I'm not a professional writer either, so I apologize for any spelling or grammar mistakes. If you find any mistakes/improvements regarding text and ofc the content, create a git issue here -> click me","title":"Wat iz dis?"},{"location":"#what-can-i-find-here","text":"All sorts of tips little tools and howto's for technical stuff. Just look around in the tabs above and on the left.","title":"What can I find here?"},{"location":"Guides/irl/","text":"IRL streaming # IRL - In REAL Life streaming has some difficulties. You have limited processing power, limited upload speed and limited battery life. Additionally mobile data caps as well as signal loss. This page will try to give you some tips and tricks to get the best out of your setup. Required Hardware # A phone with a (good) camera, in build mic, or external mic(aux/bluetooth). (USB cams are experimental on Android) A stationary PC at home with enough upload and download speed to stream. (any semi modern laptop will do probably) A bit of time to set it up. Lets get started # Download OBS for your PC install it, set it up for streaming to your favorite platform. Install the Websocket plugin for OBS. Configure a password/user as well as a random port (everything in the 4 and 5 digits should be free). Add a media source (name it whatever you want) I've used Port 22222 in this example Download \"Larix Broadcaster\" for your phone. Add a new server in the settings under connections. name can be anything, URL hast to be srt://IPOFYOURPC:PORT (PORT=22222 in this example) Delay can be lower than 2000ms, but mobile networks aren't that reliable, so I'd recommend 2000ms. Adjust the video settings to your liking 1080p 30fps is good for IRL, variable bitrate of 4kb/s, Format HEVC if available) Setup up mic settings etc Go back to the main screen and press the \"Shutter\" button. After a few seconds the video feed should be visible in your OBS Making it work from outside your home # Setup port forwarding on your router to forward the port you've chosen above to your PC running OBS. Also make the port open for your OBS websocket (You can use for example OBS Blade on your phone to control OBS remotely) Danger OPENING PORTS IS A POTENTIAL SECURITY RISK, MAKE SURE YOU KNOW WHAT YOU ARE DOING AND THAT YOU HAVE A SECURE PASSWORD FOR YOUR WEBSOCKET. ONLY LEAVE IT OPEN AS LONG AS YOU NEED IT. DON'T TELL ANYONE THE PORT NUMBERS, JUST TO BE SAFE. ANYONE WITH THE PORT AND PASSWORD OF YOUR WEBSOCKET CAN START STREAMING. YOU HAVE BEEN WARNED. Setup a Dynamic DNS service, so you have a static URL to connect to. I recommend noip.com - its free and easy to use. Create an account, add a new Domain and follow the instructions. Download the client and install it on your PC. Select your configured domain in the client Now the Dynamic DNS service Domain should always resolve in your public home address Adjust the IP in your Larix Broadcaster APP to your created DOMAIN. All done, you should now be able to stream from anywhere in the world. Cool stuff # auto scene switcher: Loopy SRT Stats Monitor Just watch his tutorial its fairly easy to follow you can minimize the Larix app to open a chat app on the same device (on android you can multi window the apps or use a floating window app) Note You can also create an overlay inside the Larix app, and set it to preview only. As url use the direct one from twitch popout chat, or JChat. Additionally you have to enable stream and record mirrored video for the selfie cam, so it doesn't flip the overlay. Pros of this setup instead of streaming directly from your phone # your phone has only to encode your camera and audio, not all overlays etc -> longer battery life you can just use all your existing overlays If your phone disconnects, the stream keeps running (with loopy you can automatically switch to a different scene, till you reconnected) Only contra: your pc has to be on during that time. -> Enjoy streaming from where ever you are without worring about disconnects etc Want to know more coll stuff related? https://obsproject.com/wiki/Streaming-With-SRT-Or-RIST-Protocols","title":"IRL streaming"},{"location":"Guides/irl/#irl-streaming","text":"IRL - In REAL Life streaming has some difficulties. You have limited processing power, limited upload speed and limited battery life. Additionally mobile data caps as well as signal loss. This page will try to give you some tips and tricks to get the best out of your setup.","title":"IRL streaming"},{"location":"Guides/irl/#required-hardware","text":"A phone with a (good) camera, in build mic, or external mic(aux/bluetooth). (USB cams are experimental on Android) A stationary PC at home with enough upload and download speed to stream. (any semi modern laptop will do probably) A bit of time to set it up.","title":"Required Hardware"},{"location":"Guides/irl/#lets-get-started","text":"Download OBS for your PC install it, set it up for streaming to your favorite platform. Install the Websocket plugin for OBS. Configure a password/user as well as a random port (everything in the 4 and 5 digits should be free). Add a media source (name it whatever you want) I've used Port 22222 in this example Download \"Larix Broadcaster\" for your phone. Add a new server in the settings under connections. name can be anything, URL hast to be srt://IPOFYOURPC:PORT (PORT=22222 in this example) Delay can be lower than 2000ms, but mobile networks aren't that reliable, so I'd recommend 2000ms. Adjust the video settings to your liking 1080p 30fps is good for IRL, variable bitrate of 4kb/s, Format HEVC if available) Setup up mic settings etc Go back to the main screen and press the \"Shutter\" button. After a few seconds the video feed should be visible in your OBS","title":"Lets get started"},{"location":"Guides/irl/#making-it-work-from-outside-your-home","text":"Setup port forwarding on your router to forward the port you've chosen above to your PC running OBS. Also make the port open for your OBS websocket (You can use for example OBS Blade on your phone to control OBS remotely) Danger OPENING PORTS IS A POTENTIAL SECURITY RISK, MAKE SURE YOU KNOW WHAT YOU ARE DOING AND THAT YOU HAVE A SECURE PASSWORD FOR YOUR WEBSOCKET. ONLY LEAVE IT OPEN AS LONG AS YOU NEED IT. DON'T TELL ANYONE THE PORT NUMBERS, JUST TO BE SAFE. ANYONE WITH THE PORT AND PASSWORD OF YOUR WEBSOCKET CAN START STREAMING. YOU HAVE BEEN WARNED. Setup a Dynamic DNS service, so you have a static URL to connect to. I recommend noip.com - its free and easy to use. Create an account, add a new Domain and follow the instructions. Download the client and install it on your PC. Select your configured domain in the client Now the Dynamic DNS service Domain should always resolve in your public home address Adjust the IP in your Larix Broadcaster APP to your created DOMAIN. All done, you should now be able to stream from anywhere in the world.","title":"Making it work from outside your home"},{"location":"Guides/irl/#cool-stuff","text":"auto scene switcher: Loopy SRT Stats Monitor Just watch his tutorial its fairly easy to follow you can minimize the Larix app to open a chat app on the same device (on android you can multi window the apps or use a floating window app) Note You can also create an overlay inside the Larix app, and set it to preview only. As url use the direct one from twitch popout chat, or JChat. Additionally you have to enable stream and record mirrored video for the selfie cam, so it doesn't flip the overlay.","title":"Cool stuff"},{"location":"Guides/irl/#pros-of-this-setup-instead-of-streaming-directly-from-your-phone","text":"your phone has only to encode your camera and audio, not all overlays etc -> longer battery life you can just use all your existing overlays If your phone disconnects, the stream keeps running (with loopy you can automatically switch to a different scene, till you reconnected) Only contra: your pc has to be on during that time. -> Enjoy streaming from where ever you are without worring about disconnects etc Want to know more coll stuff related? https://obsproject.com/wiki/Streaming-With-SRT-Or-RIST-Protocols","title":"Pros of this setup instead of streaming directly from your phone"},{"location":"Guides/InvokeAI/","text":"InvokeAI - pip only setup guide # Requirements # Python 3.9+ git Stronlgy recommended to use a GPU (This tutorial is for Nvidia GPUs, but AMD GPUs should work as well) 4gb+ VRAM 12 GB of disk a bit of time (~15min) Setup # Im going to use the build in venv module to create a virtual environment for this project. The original way of INvokeAI is using Annaconda. Look into their repo for more information. This guide is tested and working on Windows. For Linux it should work too, sorry Mac guys. Tip A \"one-click-installer\" is in the works, but not ready yet. 1. Install Python # If you have problems on this step, google is your friend :D Tip: dont forget to add python to your PATH 2. Create a virtual environment # In Order to prevent Windows file length issues, we will create the virtual environment in the root of a drive. Open a command prompt in a root folder and type: Navigation inside the command prompt To find the root of a drive, type cd / in the command prompt. To change drives, type cd <drive letter>: mkdir InvokeAI to create a folder for the project. Enter the folder and create a virtual environment: cd InvokeAI python -m venv .venv 3. Activate the virtual environment # .venv\\Scripts\\activate 4. Install InvokeAI # Get the release from the releases page and extract it into the invokeai folder. 5. Install the requirements # While the virtual environment is activated (in not see step 3 ), type: pip install -r requirements.txt 6. Download the model # Download the model from here Copy the sd-v1-4.ckpt file into the models\\ldm\\stable-diffusion-v1 folder and rename it to model.ckpt . 7. Preload the modles # python scripts/preload_models.py Start using it # python scripts\\invoke.py --gui For cli remove the --gui argument, for more see the official wiki Output directory will be output\\img-samples Updating # To update InvokeAI, just download the new release and overwrite the files and repeat the steps 5 and 7 . Or you can just use the git pull command. Notes # This setup is now fully portable, you can move the InvokeAI folder to another drive or even another computer and it will work. To make the start easier, we add a simple .bat file to the root of the InvokeAI folder (WIN only): Warning \"portable\" in a way, that it works on similar systems. If its too different, it should still work in cpu only mode. \".venv/Scripts/python.exe\" scripts\\invoke.py --gui","title":"InvokeAI - pip only setup guide"},{"location":"Guides/InvokeAI/#invokeai-pip-only-setup-guide","text":"","title":"InvokeAI - pip only setup guide"},{"location":"Guides/InvokeAI/#requirements","text":"Python 3.9+ git Stronlgy recommended to use a GPU (This tutorial is for Nvidia GPUs, but AMD GPUs should work as well) 4gb+ VRAM 12 GB of disk a bit of time (~15min)","title":"Requirements"},{"location":"Guides/InvokeAI/#setup","text":"Im going to use the build in venv module to create a virtual environment for this project. The original way of INvokeAI is using Annaconda. Look into their repo for more information. This guide is tested and working on Windows. For Linux it should work too, sorry Mac guys. Tip A \"one-click-installer\" is in the works, but not ready yet.","title":"Setup"},{"location":"Guides/InvokeAI/#1-install-python","text":"If you have problems on this step, google is your friend :D Tip: dont forget to add python to your PATH","title":"1. Install Python"},{"location":"Guides/InvokeAI/#2-create-a-virtual-environment","text":"In Order to prevent Windows file length issues, we will create the virtual environment in the root of a drive. Open a command prompt in a root folder and type: Navigation inside the command prompt To find the root of a drive, type cd / in the command prompt. To change drives, type cd <drive letter>: mkdir InvokeAI to create a folder for the project. Enter the folder and create a virtual environment: cd InvokeAI python -m venv .venv","title":"2. Create a virtual environment"},{"location":"Guides/InvokeAI/#3-activate-the-virtual-environment","text":".venv\\Scripts\\activate","title":"3. Activate the virtual environment"},{"location":"Guides/InvokeAI/#4-install-invokeai","text":"Get the release from the releases page and extract it into the invokeai folder.","title":"4. Install InvokeAI"},{"location":"Guides/InvokeAI/#5-install-the-requirements","text":"While the virtual environment is activated (in not see step 3 ), type: pip install -r requirements.txt","title":"5. Install the requirements"},{"location":"Guides/InvokeAI/#6-download-the-model","text":"Download the model from here Copy the sd-v1-4.ckpt file into the models\\ldm\\stable-diffusion-v1 folder and rename it to model.ckpt .","title":"6. Download the model"},{"location":"Guides/InvokeAI/#7-preload-the-modles","text":"python scripts/preload_models.py","title":"7. Preload the modles"},{"location":"Guides/InvokeAI/#start-using-it","text":"python scripts\\invoke.py --gui For cli remove the --gui argument, for more see the official wiki Output directory will be output\\img-samples","title":"Start using it"},{"location":"Guides/InvokeAI/#updating","text":"To update InvokeAI, just download the new release and overwrite the files and repeat the steps 5 and 7 . Or you can just use the git pull command.","title":"Updating"},{"location":"Guides/InvokeAI/#notes","text":"This setup is now fully portable, you can move the InvokeAI folder to another drive or even another computer and it will work. To make the start easier, we add a simple .bat file to the root of the InvokeAI folder (WIN only): Warning \"portable\" in a way, that it works on similar systems. If its too different, it should still work in cpu only mode. \".venv/Scripts/python.exe\" scripts\\invoke.py --gui","title":"Notes"},{"location":"Other/","text":"Other stuff # Very random stuff that doesn't fit anywhere else.","title":"Other stuff"},{"location":"Other/#other-stuff","text":"Very random stuff that doesn't fit anywhere else.","title":"Other stuff"},{"location":"Other/EStDR/","text":"EStDR - Epidemic Sound to DaVinci Resolve # What is EStDR? # Its a simple tool (Direfox Extension + Python middle man) that allows you to import Epidemic Sound tracks direclty into DaVinci Resolve + saves them into a folder of your choice. Info This is heavily work in progress, so expect bugs and issues and missing features. How to install it? # Download the latest release from the releases page . You only need the EStDR.xdi for now. Simply drag and drop it onto your browser and it will prompt you to install it. Now download the server.zip from the same page and extract it somewhere. Info Required is Python3 and the websockets module. You can install it with pip install websockets . Before running the server, you need to edit the ws.py file. (Will be changed in the future). Open the file and change the path variable to the path of the desired folder where the tracks should be saved. Now you can run the ws.py file. Its still a lillte buggy, maybe you have to go to the extention settings and restart it, to make it connect. How to use it? # Go to any Track page or playlist/album page on Epidemic Sound and click the EStDR icon in your browser. IT will download all Tracks into the folder and imports them into the mediapool. Planned features # some sort settings - ui? - in the browser/external window? directly import into timeline? Better docs -> short gif?","title":"EStDR - Epidemic Sound to DaVinci Resolve"},{"location":"Other/EStDR/#estdr-epidemic-sound-to-davinci-resolve","text":"","title":"EStDR - Epidemic Sound to DaVinci Resolve"},{"location":"Other/EStDR/#what-is-estdr","text":"Its a simple tool (Direfox Extension + Python middle man) that allows you to import Epidemic Sound tracks direclty into DaVinci Resolve + saves them into a folder of your choice. Info This is heavily work in progress, so expect bugs and issues and missing features.","title":"What is EStDR?"},{"location":"Other/EStDR/#how-to-install-it","text":"Download the latest release from the releases page . You only need the EStDR.xdi for now. Simply drag and drop it onto your browser and it will prompt you to install it. Now download the server.zip from the same page and extract it somewhere. Info Required is Python3 and the websockets module. You can install it with pip install websockets . Before running the server, you need to edit the ws.py file. (Will be changed in the future). Open the file and change the path variable to the path of the desired folder where the tracks should be saved. Now you can run the ws.py file. Its still a lillte buggy, maybe you have to go to the extention settings and restart it, to make it connect.","title":"How to install it?"},{"location":"Other/EStDR/#how-to-use-it","text":"Go to any Track page or playlist/album page on Epidemic Sound and click the EStDR icon in your browser. IT will download all Tracks into the folder and imports them into the mediapool.","title":"How to use it?"},{"location":"Other/EStDR/#planned-features","text":"some sort settings - ui? - in the browser/external window? directly import into timeline? Better docs -> short gif?","title":"Planned features"},{"location":"Other/WhatsappEfixFixer/","text":"Whatsapp Efix Fixer # Uhm What da heck is that even? # Basically every file, has the creation date baked into the file itself. With this information a gallery for example can order Images based on \"taken\" dat, even if the file name has nothing to do with the date. Whatsapp-backups just backup the file WITHOUT the exif, why? I don't know, these few bytes wouldn't hurt anyone. Maybe you restored a Whatsapp-backup one day, and noticed that all Images from Whatsapp got created today, and therefore the Gallery is totally fucked up. Luckily whatsapp names the images and videos based on the creation date, so we can just add the exif data back in, with a simple script. The script # What does it do excactly? It goes through every image in the \"process_images\" folder, and adds the exif data back to it. If multiple files got the same date, an incrementing counter will be added to the filename. To use it, you can either map the working folder to the actual image/video/media folder on you connected phone, or copy the files onto the computer, and then run the script, and copy them back. That way you also have an backup of the original files incase something goes wrong. (The online Backup could become unusable, if it automatically backs up during the process) whatsappexiffixer.py from datetime import datetime import piexif import os import time errors_count = 0 errors_names = [] folder = \"./process_images/\" def get_datetime ( filename ): date_str = filename . split ( \"-\" )[ 1 ] return datetime . strptime ( date_str , \"%Y%m %d \" ) def get_date ( filename ): date_str = filename . split ( \"-\" )[ 1 ] date_str2 = filename . split ( \"-\" )[ 2 ][ 4 : 6 ] if date_str2 == \"00\" or int ( date_str2 ) >= 24 : return datetime . strptime ( date_str , \"%Y%m %d \" ) . strftime ( \"%Y:%m: %d %H:%M:%S\" ) else : return datetime . strptime ( date_str + date_str2 , \"%Y%m %d %H\" ) . strftime ( \"%Y:%m: %d %H:%M:%S\" ) allowedFileEndings = [ \"mp4\" , \"jpg\" , \"3gp\" , \"jpeg\" ] filenames = [ fn for fn in os . listdir ( folder ) if fn . split ( \".\" )[ - 1 ] in allowedFileEndings ] num_files = len ( filenames ) print ( \"Number of files: {} \" . format ( num_files )) for i , filename in enumerate ( filenames ): try : if filename . endswith ( \"mp4\" ) or filename . endswith ( \"3gp\" ): date = get_datetime ( filename ) modTime = time . mktime ( date . timetuple ()) os . utime ( folder + filename , ( modTime , modTime )) elif filename . endswith ( \"jpg\" ) or filename . endswith ( \"jpeg\" ): exif_dict = { \"Exif\" : { piexif . ExifIFD . DateTimeOriginal : get_date ( filename )}} exif_bytes = piexif . dump ( exif_dict ) piexif . insert ( exif_bytes , folder + filename ) num_digits = len ( str ( num_files )) print ( \"{num: {width} }/ {max} - {filename} \" . format ( num = i + 1 , width = num_digits , max = num_files , filename = folder + filename ) ) except : errors_count = errors_count + 1 errors_names . append ( filename ) pass print ( \" \\n Done!\" ) print ( \" \\n Erroes:\" + str ( errors_count ) + \"!\" ) print ( f \" \\n { errors_names } \" ) Danger This script worked for me a few months ago, maybe stuff has changed since then. So as always, it comes as is and always read code you found online before you run it. Im not responsible for any damage this script might cause.","title":"Whatsapp Efix Fixer"},{"location":"Other/WhatsappEfixFixer/#whatsapp-efix-fixer","text":"","title":"Whatsapp Efix Fixer"},{"location":"Other/WhatsappEfixFixer/#uhm-what-da-heck-is-that-even","text":"Basically every file, has the creation date baked into the file itself. With this information a gallery for example can order Images based on \"taken\" dat, even if the file name has nothing to do with the date. Whatsapp-backups just backup the file WITHOUT the exif, why? I don't know, these few bytes wouldn't hurt anyone. Maybe you restored a Whatsapp-backup one day, and noticed that all Images from Whatsapp got created today, and therefore the Gallery is totally fucked up. Luckily whatsapp names the images and videos based on the creation date, so we can just add the exif data back in, with a simple script.","title":"Uhm What da heck is that even?"},{"location":"Other/WhatsappEfixFixer/#the-script","text":"What does it do excactly? It goes through every image in the \"process_images\" folder, and adds the exif data back to it. If multiple files got the same date, an incrementing counter will be added to the filename. To use it, you can either map the working folder to the actual image/video/media folder on you connected phone, or copy the files onto the computer, and then run the script, and copy them back. That way you also have an backup of the original files incase something goes wrong. (The online Backup could become unusable, if it automatically backs up during the process) whatsappexiffixer.py from datetime import datetime import piexif import os import time errors_count = 0 errors_names = [] folder = \"./process_images/\" def get_datetime ( filename ): date_str = filename . split ( \"-\" )[ 1 ] return datetime . strptime ( date_str , \"%Y%m %d \" ) def get_date ( filename ): date_str = filename . split ( \"-\" )[ 1 ] date_str2 = filename . split ( \"-\" )[ 2 ][ 4 : 6 ] if date_str2 == \"00\" or int ( date_str2 ) >= 24 : return datetime . strptime ( date_str , \"%Y%m %d \" ) . strftime ( \"%Y:%m: %d %H:%M:%S\" ) else : return datetime . strptime ( date_str + date_str2 , \"%Y%m %d %H\" ) . strftime ( \"%Y:%m: %d %H:%M:%S\" ) allowedFileEndings = [ \"mp4\" , \"jpg\" , \"3gp\" , \"jpeg\" ] filenames = [ fn for fn in os . listdir ( folder ) if fn . split ( \".\" )[ - 1 ] in allowedFileEndings ] num_files = len ( filenames ) print ( \"Number of files: {} \" . format ( num_files )) for i , filename in enumerate ( filenames ): try : if filename . endswith ( \"mp4\" ) or filename . endswith ( \"3gp\" ): date = get_datetime ( filename ) modTime = time . mktime ( date . timetuple ()) os . utime ( folder + filename , ( modTime , modTime )) elif filename . endswith ( \"jpg\" ) or filename . endswith ( \"jpeg\" ): exif_dict = { \"Exif\" : { piexif . ExifIFD . DateTimeOriginal : get_date ( filename )}} exif_bytes = piexif . dump ( exif_dict ) piexif . insert ( exif_bytes , folder + filename ) num_digits = len ( str ( num_files )) print ( \"{num: {width} }/ {max} - {filename} \" . format ( num = i + 1 , width = num_digits , max = num_files , filename = folder + filename ) ) except : errors_count = errors_count + 1 errors_names . append ( filename ) pass print ( \" \\n Done!\" ) print ( \" \\n Erroes:\" + str ( errors_count ) + \"!\" ) print ( f \" \\n { errors_names } \" ) Danger This script worked for me a few months ago, maybe stuff has changed since then. So as always, it comes as is and always read code you found online before you run it. Im not responsible for any damage this script might cause.","title":"The script"},{"location":"Tests/encoding/","text":"Encoder comparison # If you found that page, its more or less clear that you already know about OBS and the basics (Otherwise there are plenty of great tutorials out there). I'Ve tested all following with version 28.0.3 (64bit) but since those are encoder settings, they should work with any version. Best settings for Twitch? # Okay first, what does Twitch recommend? reference 1080p60 NVENC x264 Bitrare cbr 6000 kbps cbr 6000 kbps preset: Quality veryfast <-> medium Profile NaN Main/High *Nvenc = NVIDIA accelerated encoding on GPU | x264 = software encoding (CPU) It's 2022 so im not gonna really bother testing lower resolutions, but it should mostly match the upcoming outcome. Testing method # Since the image quality is pretty close and we as humans cant watch to videos simultaneously, I'm relying on software to compare the encoders with different settings. As Software tool I came across VMAF (Video Multimethod Assessment Fusion), made by Netflix. It tries to measure the perceptual video quality by comparing it to a reference video. The maintainers of the GitHub repo were kind and linked a functional GUI by fifonik called FFMetrics . *before using FFMetrics you have to install ffmpeg and add it to your Path **I've used 1.3.1 beta 2 for this test. We need a reference video # This video should has a ~much higher bitrate than the target bitrate. For us the target is 6kb/s, 20kb/s should be fine, my test video had vbr_50kb/s+. Additionally my video was 4k but that doesnt matter for the test. (Also downscaled it to 1080p with 70kb/s_vbr to test against, same result) Encode the videos # Im using Adobe MediaEncoder (2022) for this, with the voukoder plugin. Any other software with NVENC support should work as well. (Handbrake, or bare bones ffmpeg works) Encoder set to CBR 6kb/s presets visible in name. Note: X264 tries to break out of the 6kb/s limit (not sure why, 7kb/s would be still fine for twitch, and in real world bitrate will vary a bit anyway), just to keep a little in mind when comparing the results. To optimize NVENC I've used following: b=6000000 b_adapt=1 bf=2 nonref_p=1 preset=p7 profile=high rc=cbr spatial-aq=1 temporal-aq=1 voukoder/ffmpeg settings mapped to obs b = bitrate | b_adapt = Look-ahead | bf = Max B-frames | preset = p7 equals\"Max quality\" | profile = profile | rc = rate control | spatial-aq and temporal-aq = are both under \"Psycho Visual Tuning\" *kb/s are average over the whole 20 seconds Results # Why did I \"optimize\"/ took extra steps for NVENC? well look without and with: Software x264 is better in such difficult scenarios with limited bitrate: But there is still a big downside to Software x264, it's running completely on the CPU. My highest used setting was \"slower\" (there ist still very slow and placebo), but slower was already the point where the CPU was at 100% usage, so realtime rendering without hiccups is not possible, playing a game next to it is not possible. My CPU is a Ryzen 5 3600 on 4.2ghz, but even with more cores its not really justifiable to use x264 for streaming, except for a dual system configuration. Alright, Whats the real world difference? (gaming) # For that comparison I recorded a 20 second clip of Apex walking, spinning the camera like an idiot and shooting like in a fight. (Source record was cbr_20kb/s, 1080p, 60fps) We can see that the NVENC encoder falls a bit short during high motion scenes, but considered that the x264 is only realtime while maxing out all 12 threads of my CPU, NVENC is still the winner. Quality during walking, sliding, looking around is pretty much the same. Between frame 1130 and 1200 the NVENC encoder really fucks up compared to x264, but actually to a human it looks more or less the same (the crazy cam spinning). With x264 profile set to superfast (so I could actually stream and game, next to it) the quality is even worse (yellow line) and gets easily outperformed by NVENC. Technical results # Additionally avoid the faster presets, because they vary from frame to frame with their quality, as seen in the following graph. Im guessing, the spikes are the B-Frames and the intermediate frames are with a lower quality. Hold up - there is AV1 coming???? # Yes, but you can completely forget software encoding - for the same quality of x264 you need at least 5 times the render time. Intel Arc and the GeForce 4000 and probably upwards cards support AV1 hardware encoding, but none of the streaming providers support it yet. There is barely support for decoding (RTX 3000 supports its). And What about the framerate? # Good thought, I thought about that too, like 6kb/s on 48fps or even 30 - 30, that should be doubled the quality with the same bitrate, right? Nope, not at all. I've run test with different framerates (60 high bitrate to 60 with 6kb/s, to 48 and to 30). The results were pretty much the same, almost negligible difference in the real world. Speaking of maybe max 1% difference, but that is not really noticeable, plus with 48fps, if you want to record/ download your VOD and cut it, upload it to youtube, you will probably use 60, so frameinterpolation - and thats not worth it. Simply stick with 60fps. Conclusion # NVENC is the way to go, if you have a NVidia GPU. AMD has a similar feature called AMF (i think its called?), but I've never tested it myself, should perform about the same. Not sure if the extra options (Psycho Visual Tuning and Look-ahead) are available for AMD, but there is probably something similar. If you have the spare cpu power (meaning probably a second pc or 12cores+), its worth a consideration to squeeze out a little more quality, but simply said, go with GPU accelerated encoding.","title":"Encoder comparison"},{"location":"Tests/encoding/#encoder-comparison","text":"If you found that page, its more or less clear that you already know about OBS and the basics (Otherwise there are plenty of great tutorials out there). I'Ve tested all following with version 28.0.3 (64bit) but since those are encoder settings, they should work with any version.","title":"Encoder comparison"},{"location":"Tests/encoding/#best-settings-for-twitch","text":"Okay first, what does Twitch recommend? reference 1080p60 NVENC x264 Bitrare cbr 6000 kbps cbr 6000 kbps preset: Quality veryfast <-> medium Profile NaN Main/High *Nvenc = NVIDIA accelerated encoding on GPU | x264 = software encoding (CPU) It's 2022 so im not gonna really bother testing lower resolutions, but it should mostly match the upcoming outcome.","title":"Best settings for Twitch?"},{"location":"Tests/encoding/#testing-method","text":"Since the image quality is pretty close and we as humans cant watch to videos simultaneously, I'm relying on software to compare the encoders with different settings. As Software tool I came across VMAF (Video Multimethod Assessment Fusion), made by Netflix. It tries to measure the perceptual video quality by comparing it to a reference video. The maintainers of the GitHub repo were kind and linked a functional GUI by fifonik called FFMetrics . *before using FFMetrics you have to install ffmpeg and add it to your Path **I've used 1.3.1 beta 2 for this test.","title":"Testing method"},{"location":"Tests/encoding/#we-need-a-reference-video","text":"This video should has a ~much higher bitrate than the target bitrate. For us the target is 6kb/s, 20kb/s should be fine, my test video had vbr_50kb/s+. Additionally my video was 4k but that doesnt matter for the test. (Also downscaled it to 1080p with 70kb/s_vbr to test against, same result)","title":"We need a reference video"},{"location":"Tests/encoding/#encode-the-videos","text":"Im using Adobe MediaEncoder (2022) for this, with the voukoder plugin. Any other software with NVENC support should work as well. (Handbrake, or bare bones ffmpeg works) Encoder set to CBR 6kb/s presets visible in name. Note: X264 tries to break out of the 6kb/s limit (not sure why, 7kb/s would be still fine for twitch, and in real world bitrate will vary a bit anyway), just to keep a little in mind when comparing the results. To optimize NVENC I've used following: b=6000000 b_adapt=1 bf=2 nonref_p=1 preset=p7 profile=high rc=cbr spatial-aq=1 temporal-aq=1 voukoder/ffmpeg settings mapped to obs b = bitrate | b_adapt = Look-ahead | bf = Max B-frames | preset = p7 equals\"Max quality\" | profile = profile | rc = rate control | spatial-aq and temporal-aq = are both under \"Psycho Visual Tuning\" *kb/s are average over the whole 20 seconds","title":"Encode the videos"},{"location":"Tests/encoding/#results","text":"Why did I \"optimize\"/ took extra steps for NVENC? well look without and with: Software x264 is better in such difficult scenarios with limited bitrate: But there is still a big downside to Software x264, it's running completely on the CPU. My highest used setting was \"slower\" (there ist still very slow and placebo), but slower was already the point where the CPU was at 100% usage, so realtime rendering without hiccups is not possible, playing a game next to it is not possible. My CPU is a Ryzen 5 3600 on 4.2ghz, but even with more cores its not really justifiable to use x264 for streaming, except for a dual system configuration.","title":"Results"},{"location":"Tests/encoding/#alright-whats-the-real-world-difference-gaming","text":"For that comparison I recorded a 20 second clip of Apex walking, spinning the camera like an idiot and shooting like in a fight. (Source record was cbr_20kb/s, 1080p, 60fps) We can see that the NVENC encoder falls a bit short during high motion scenes, but considered that the x264 is only realtime while maxing out all 12 threads of my CPU, NVENC is still the winner. Quality during walking, sliding, looking around is pretty much the same. Between frame 1130 and 1200 the NVENC encoder really fucks up compared to x264, but actually to a human it looks more or less the same (the crazy cam spinning). With x264 profile set to superfast (so I could actually stream and game, next to it) the quality is even worse (yellow line) and gets easily outperformed by NVENC.","title":"Alright, Whats the real world difference? (gaming)"},{"location":"Tests/encoding/#technical-results","text":"Additionally avoid the faster presets, because they vary from frame to frame with their quality, as seen in the following graph. Im guessing, the spikes are the B-Frames and the intermediate frames are with a lower quality.","title":"Technical results"},{"location":"Tests/encoding/#hold-up-there-is-av1-coming","text":"Yes, but you can completely forget software encoding - for the same quality of x264 you need at least 5 times the render time. Intel Arc and the GeForce 4000 and probably upwards cards support AV1 hardware encoding, but none of the streaming providers support it yet. There is barely support for decoding (RTX 3000 supports its).","title":"Hold up - there is AV1 coming????"},{"location":"Tests/encoding/#and-what-about-the-framerate","text":"Good thought, I thought about that too, like 6kb/s on 48fps or even 30 - 30, that should be doubled the quality with the same bitrate, right? Nope, not at all. I've run test with different framerates (60 high bitrate to 60 with 6kb/s, to 48 and to 30). The results were pretty much the same, almost negligible difference in the real world. Speaking of maybe max 1% difference, but that is not really noticeable, plus with 48fps, if you want to record/ download your VOD and cut it, upload it to youtube, you will probably use 60, so frameinterpolation - and thats not worth it. Simply stick with 60fps.","title":"And What about the framerate?"},{"location":"Tests/encoding/#conclusion","text":"NVENC is the way to go, if you have a NVidia GPU. AMD has a similar feature called AMF (i think its called?), but I've never tested it myself, should perform about the same. Not sure if the extra options (Psycho Visual Tuning and Look-ahead) are available for AMD, but there is probably something similar. If you have the spare cpu power (meaning probably a second pc or 12cores+), its worth a consideration to squeeze out a little more quality, but simply said, go with GPU accelerated encoding.","title":"Conclusion"},{"location":"Troubleshooting/Voicemeeter/","text":"Voicemeeter # The all-in-one audio mixer solution for your PC made by VB-Audio Software.","title":"Voicemeeter"},{"location":"Troubleshooting/Voicemeeter/#voicemeeter","text":"The all-in-one audio mixer solution for your PC made by VB-Audio Software.","title":"Voicemeeter"},{"location":"Troubleshooting/Voicemeeter/mic_crackle/","text":"Mic crunshing / crackling # Open the sound settings and set all your devices which connect to VoiceMeeter to 48khz. win+r a window should pop out on the bottom left enter control mmsys.cpl sounds and click OK or press enter Inside Voicemeeter set the Engine to 48khz as well. As a last step, restart the Voicemeeter Engine (Menu -> First item)","title":"Mic crunshing / crackling"},{"location":"Troubleshooting/Voicemeeter/mic_crackle/#mic-crunshing-crackling","text":"Open the sound settings and set all your devices which connect to VoiceMeeter to 48khz. win+r a window should pop out on the bottom left enter control mmsys.cpl sounds and click OK or press enter Inside Voicemeeter set the Engine to 48khz as well. As a last step, restart the Voicemeeter Engine (Menu -> First item)","title":"Mic crunshing / crackling"}]}